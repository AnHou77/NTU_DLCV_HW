{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.models.inception import inception_v3\n",
    "from torch.autograd import Variable\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper parameters ##\n",
    "image_size = 64\n",
    "channel_size = 3\n",
    "lv_size = 100\n",
    "G_fs = 64\n",
    "D_fs = 64\n",
    "lr_G = 0.0002\n",
    "lr_D = 0.0004\n",
    "beta1 = 0.5\n",
    "epochs = 600\n",
    "save_model_dir = './models/'\n",
    "manualSeed = 6\n",
    "#######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class p1(Dataset):\n",
    "    def __init__(self, root):\n",
    "\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "        self.filenames = []\n",
    "        self.fileindices = []\n",
    "        self.root = root\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size,image_size)),\n",
    "            # transforms.Resize((int(image_size * 1.2), int(image_size * 1.2))),\n",
    "            # transforms.CenterCrop((image_size, image_size)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        filenames = glob.glob(os.path.join(root, '*.png'))\n",
    "        self.filenames = sorted(filenames)\n",
    "        for fn in self.filenames:\n",
    "            fn = fn.replace(root,\"\")\n",
    "            self.fileindices.append(fn.replace(\"/\",\"\"))\n",
    "                \n",
    "        self.len = len(self.filenames)\n",
    "                              \n",
    "    def __getitem__(self, index):\n",
    "        image_fn = self.filenames[index]\n",
    "        image = Image.open(image_fn)\n",
    "            \n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # input (latent vector z)\n",
    "            nn.ConvTranspose2d(lv_size, G_fs * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(G_fs * 8),\n",
    "            # nn.LeakyReLU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(0.5),\n",
    "            # state size. (G_feature_size*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(G_fs * 8, G_fs * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(G_fs * 4),\n",
    "            # nn.LeakyReLU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(0.5),\n",
    "            # state size. (G_feature_size*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(G_fs * 4, G_fs * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(G_fs * 2),\n",
    "            # nn.LeakyReLU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(0.5),\n",
    "            # state size. (G_feature_size*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(G_fs * 2, G_fs, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(G_fs),\n",
    "            # nn.LeakyReLU(inplace=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(0.5),\n",
    "            # state size. (G_feature_size) x 32 x 32\n",
    "            nn.ConvTranspose2d(G_fs, channel_size, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (channel_size) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # input (channel_size x 64 x 64)\n",
    "            nn.Conv2d(channel_size, D_fs, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (D_feature_size) x 32 x 32\n",
    "            nn.Conv2d(D_fs, D_fs * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(D_fs * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (D_feature_size*2) x 16 x 16\n",
    "            nn.Conv2d(D_fs * 2, D_fs * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(D_fs * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (D_feature_size*4) x 8 x 8\n",
    "            nn.Conv2d(D_fs * 4, D_fs * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(D_fs * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (D_feature_size*8) x 4 x 4\n",
    "            nn.Conv2d(D_fs * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(model.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(model.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(model.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_score(imgs, cuda=True, batch_size=32, resize=False, splits=1):\n",
    "    \"\"\"Computes the inception score of the generated images imgs\n",
    "    imgs -- Torch dataset of (3xHxW) numpy images normalized in the range [-1, 1]\n",
    "    cuda -- whether or not to run on GPU\n",
    "    batch_size -- batch size for feeding into Inception v3\n",
    "    splits -- number of splits\n",
    "    \"\"\"\n",
    "    N = len(imgs)\n",
    "\n",
    "    assert batch_size > 0\n",
    "    assert N > batch_size\n",
    "\n",
    "    # Set up dtype\n",
    "    if cuda:\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"WARNING: You have a CUDA device, so you should probably set cuda=True\")\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    # Set up dataloader\n",
    "    dataloader = DataLoader(imgs, batch_size=batch_size)\n",
    "\n",
    "    # Load inception model\n",
    "    inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)\n",
    "    inception_model.eval()\n",
    "    up = nn.Upsample(size=(299, 299), mode='bilinear').type(dtype)\n",
    "    def get_pred(x):\n",
    "        if resize:\n",
    "            x = up(x)\n",
    "        x = inception_model(x)\n",
    "        return F.softmax(x).data.cpu().numpy()\n",
    "\n",
    "    # Get predictions\n",
    "    preds = np.zeros((N, 1000))\n",
    "\n",
    "    for i, batch in enumerate(dataloader, 0):\n",
    "        batch = batch.type(dtype)\n",
    "        batchv = Variable(batch)\n",
    "        batch_size_i = batch.size()[0]\n",
    "\n",
    "        preds[i*batch_size:i*batch_size + batch_size_i] = get_pred(batchv)\n",
    "\n",
    "    # Now compute the mean kl-div\n",
    "    split_scores = []\n",
    "\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(entropy(pyx, py))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "    return np.mean(split_scores), np.std(split_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manualSeed = 6\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.cuda.manual_seed_all(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = p1(root='hw2_data/face/train')\n",
    "\n",
    "print('# images in trainset:', len(trainset))\n",
    "\n",
    "trainset_loader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "dataiter = iter(trainset_loader)\n",
    "images = dataiter.next()\n",
    "print('(Trainset) Image tensor in each batch:', images.shape, images.dtype)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(images.to(device), nrow=16,padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_model = Generator().to(device)\n",
    "G_model.apply(weights_init)\n",
    "print(G_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_model = Discriminator().to(device)\n",
    "D_model.apply(weights_init)\n",
    "print(D_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize the progression of the generator\n",
    "fixed_noise = torch.randn(64, lv_size, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "G_optimizer = optim.Adam(G_model.parameters(), lr=lr_G, betas=(beta1, 0.999))\n",
    "D_optimizer = optim.Adam(D_model.parameters(), lr=lr_D, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(data_root):\n",
    "\n",
    "    # use fixed random seed\n",
    "    print(\"Random Seed: \", manualSeed)\n",
    "    np.random.seed(manualSeed)\n",
    "    random.seed(manualSeed)\n",
    "    torch.manual_seed(manualSeed)\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "    trainset = p1(root=data_root)\n",
    "\n",
    "    print('# images in trainset:', len(trainset))\n",
    "\n",
    "    trainset_loader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "    dataiter = iter(trainset_loader)\n",
    "    images = dataiter.next()\n",
    "    print('(Trainset) Image tensor in each batch:', images.shape, images.dtype)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "    G_model = Generator().to(device)\n",
    "    G_model.apply(weights_init)\n",
    "\n",
    "    D_model = Discriminator().to(device)\n",
    "    D_model.apply(weights_init)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    real_label = 1\n",
    "    fake_label = 0\n",
    "\n",
    "    G_optimizer = optim.Adam(G_model.parameters(), lr=lr_G, betas=(beta1, 0.999))\n",
    "    D_optimizer = optim.Adam(D_model.parameters(), lr=lr_D, betas=(beta1, 0.999))\n",
    "\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "\n",
    "    data_size = len(trainset_loader)\n",
    "\n",
    "    D_model.train()\n",
    "    G_model.train()\n",
    "\n",
    "    print(\"Start Training ...\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        G_loss = 0.0\n",
    "        D_loss = 0.0\n",
    "\n",
    "        D_x_sum = 0.0\n",
    "        D_G_z1_sum = 0.0\n",
    "        D_G_z2_sum = 0.0\n",
    "\n",
    "        for _, (data) in enumerate(tqdm(trainset_loader)):\n",
    "            data = data.to(device)\n",
    "\n",
    "            ##ã€€Training Discriminator: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            D_model.zero_grad()\n",
    "            \n",
    "            b_size = data.size(0)\n",
    "            label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "\n",
    "            # Train with real images\n",
    "            output = D_model(data).view(-1)\n",
    "\n",
    "            D_real_loss = criterion(output, label)\n",
    "            D_real_loss.backward()\n",
    "            \n",
    "            # D(x)\n",
    "            D_x = output.mean().item()\n",
    "            D_x_sum += D_x\n",
    "\n",
    "            ## Train with fake images\n",
    "            noise = torch.randn(b_size, lv_size, 1, 1, device=device)\n",
    "\n",
    "            fake = G_model(noise)\n",
    "            label.fill_(fake_label)\n",
    "\n",
    "            output = D_model(fake.detach()).view(-1)\n",
    "\n",
    "            D_fake_loss = criterion(output, label)\n",
    "            D_fake_loss.backward()\n",
    "\n",
    "            # D(G(z)) before update D\n",
    "            D_G_z1 = output.mean().item()\n",
    "            D_G_z1_sum += D_G_z1\n",
    "            # Discriminator loss = loss from training with real images + loss from training with fake images\n",
    "            loss_D = D_real_loss + D_fake_loss\n",
    "\n",
    "            D_loss += loss_D.item()\n",
    "            # Update D optimizer\n",
    "            D_optimizer.step()\n",
    "\n",
    "            \n",
    "            # Training Generator: maximize log(D(G(z)))\n",
    "            G_model.zero_grad()\n",
    "            label.fill_(real_label)\n",
    "\n",
    "            output = D_model(fake).view(-1)\n",
    "\n",
    "            loss_G = criterion(output, label)\n",
    "            G_loss += loss_G.item()\n",
    "            loss_G.backward()\n",
    "\n",
    "            # D(G(z)) after update D\n",
    "            D_G_z2 = output.mean().item()\n",
    "            D_G_z2_sum += D_G_z2\n",
    "            # Update G optimizer\n",
    "            G_optimizer.step()\n",
    "\n",
    "        G_loss = G_loss / data_size\n",
    "        D_loss = D_loss / data_size\n",
    "\n",
    "        D_x = D_x_sum / data_size\n",
    "        D_G_z1 = D_G_z1_sum / data_size\n",
    "        D_G_z2 = D_G_z2_sum / data_size\n",
    "\n",
    "        print(f\"[ Train | {epoch:03d}/{epochs:03d} ] G_loss = {G_loss:.4f}, D_loss = {D_loss:.4f}, D(x) = {D_x:.4f}, D(G(z)) before update D = {D_G_z1:.4f}, D(G(z)) after update D = {D_G_z2:.4f}\")\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(G_loss)\n",
    "        D_losses.append(D_loss)\n",
    "\n",
    "        G_model.eval()\n",
    "        noise = torch.randn(1000, lv_size, 1, 1, device=device)\n",
    "\n",
    "        i_s = inception_score(G_model(noise).detach().cpu(), cuda=True, resize=True, batch_size=50)[0]\n",
    "        print(f'IS: {i_s:.4f}')\n",
    "\n",
    "        if (D_loss <= 0.0001) or (D_loss >= 50.0):\n",
    "            break\n",
    "        \n",
    "        # if inception score larger than strong baseline, then save model.\n",
    "        if (epoch >= 100) and (i_s >= 2.15):\n",
    "            torch.save(G_model.state_dict(), os.path.join(save_model_dir,f'g_model_bs128_flip_{epoch}_{i_s}.pth'))\n",
    "\n",
    "    return G_losses, D_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_losses, D_losses = training('hw2_data/face/train')\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "G1 = Generator().to(device)\n",
    "G1.load_state_dict(torch.load(os.path.join(save_model_dir,'dcgan_generator_584ep.pth')))\n",
    "manualSeed = 6\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.cuda.manual_seed_all(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1.eval()\n",
    "noise = torch.randn(1000, lv_size, 1, 1, device=device)\n",
    "fake = G1(noise)\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(fake.detach().cpu()[0:32], nrow=8, padding=2, normalize=True),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img in enumerate(fake):\n",
    "    vutils.save_image(img, os.path.join('./save_images', f'{i:04d}.jpg'), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Calculating Inception Score...\")\n",
    "is_score = inception_score(G1(noise).detach().cpu(), cuda=True, resize=True, batch_size=50)[0]\n",
    "print(f'IS: {is_score:.4f}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db60f8bc498ffbff818a03c5fdb81e3c5ac52d758a89dc54e56559b7d87491c5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
